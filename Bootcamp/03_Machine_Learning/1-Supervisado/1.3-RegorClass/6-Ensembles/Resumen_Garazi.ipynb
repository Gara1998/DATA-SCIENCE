{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) VOTING CLASSIFIER\n",
    "\n",
    "Se entrenan varios modelos (pueden ser diferentes algoritmo o iguales con diferentes parametros) y aplicando el hard o soft voting obtenemos un resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./Teoria/data/titanic_modified.csv')\n",
    "X=df[['Pclass','Sex','Age','SibSp','Parch']]\n",
    "y=df['Survived']\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7988826815642458\n",
      "Accuracy Train:  0.8806179775280899\n"
     ]
    }
   ],
   "source": [
    "#Creamos los objetos\n",
    "logistic_reg=LogisticRegression(random_state=22)\n",
    "tree_1=DecisionTreeClassifier(max_depth=8, random_state=22)\n",
    "forest=RandomForestClassifier(n_estimators=10, max_depth=8, random_state=22)\n",
    "\n",
    "#Estimators: etiquetamos los objetos creados\n",
    "estimators=[('Logistica', logistic_reg),('Decision_Tree', tree_1),('Decision_Forest',forest)]\n",
    "\n",
    "#Combinamos los objetos y decidimos manera de votacion (hard/soft)\n",
    "voting_clasi=VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "#Entrenamiento\n",
    "voting_clasi.fit(x_train,y_train)\n",
    "\n",
    "#test\n",
    "y_pred=voting_clasi.predict(x_test)\n",
    "\n",
    "#Resultado: acuracy\n",
    "print('Accuracy Test: ', accuracy_score(y_test,y_pred))\n",
    "y_pred_train=voting_clasi.predict(x_train)\n",
    "print('Accuracy Train: ', accuracy_score(y_train,y_pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos visualizar el resultado que obtiene cada algoritmo por separado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.7877094972067039\n",
      "DecisionTreeClassifier 0.7988826815642458\n",
      "RandomForestClassifier 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "for algoritmo in (logistic_reg,tree_1,forest):\n",
    "    algoritmo.fit(x_train,y_train)\n",
    "    y_pred=algoritmo.predict(x_test)\n",
    "    print(algoritmo.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) BAGGING CLASSIFIER\n",
    "\n",
    "Sistema de clasificacion por votaci√≥n de algoritmos. Se comparan algoritmos de mismo tipo. Lo que hace es dividir los datos y con cada uno gace un algoritmo (podemos dividir el set de datos con reemplazamiento (bagging) o sin reemplazamiento (pasting). Mejor bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7597765363128491\n",
      "Accuracy Train:  0.8356741573033708\n"
     ]
    }
   ],
   "source": [
    "#Creamos el algoritmo que queremos usar\n",
    "tree=DecisionTreeClassifier(max_depth=4, random_state=22)\n",
    "\n",
    "#Creamos el objeto de bagging\n",
    "bagging=BaggingClassifier(estimator=tree,\n",
    "                          n_estimators=10, #En cuantos subconjuntos dividimos el set de datos. Cada subconjunto un algoritmo.\n",
    "                          max_samples=100, #Numero maximo de datos para cada stimator\n",
    "                          bootstrap=True, #Bagging (reemplazo). False (sin reemplazo): pasting\n",
    "                          random_state=22)\n",
    "\n",
    "#Entrenamiento\n",
    "bagging.fit(x_train,y_train)\n",
    "\n",
    "#Test\n",
    "y_pred=bagging.predict(x_test)\n",
    "\n",
    "#Resultado: acuracy\n",
    "print('Accuracy Test: ', accuracy_score(y_test,y_pred))\n",
    "y_pred_train=bagging.predict(x_train)\n",
    "print('Accuracy Train: ', accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varios decion tree. Seria una mezcla de decision tree y Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7877094972067039\n",
      "Accuracy Train:  0.8525280898876404\n"
     ]
    }
   ],
   "source": [
    "# Creamos el objeto\n",
    "forest=RandomForestClassifier(n_estimators=150,\n",
    "                              max_leaf_nodes=16,\n",
    "                              random_state=22)\n",
    "\n",
    "#Entrenamiento\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#Test\n",
    "y_pred=forest.predict(x_test)\n",
    "\n",
    "#Resultado: acuracy\n",
    "print('Accuracy Test: ', accuracy_score(y_test,y_pred))\n",
    "y_pred_train=forest.predict(x_train)\n",
    "print('Accuracy Train: ', accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) ADABOOST\n",
    "\n",
    "Boosting. Los algoritmos se construyen de manera secuencial y no de manera independiente como los vistos hasta este punto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7821229050279329\n",
      "Accuracy Train:  0.9157303370786517\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo (estimator) que queremos usar\n",
    "tree=DecisionTreeClassifier(max_depth=4, random_state=22)\n",
    "\n",
    "#Creamos el objeto de Boosting (adaboost)\n",
    "ada=AdaBoostClassifier(estimator=tree,\n",
    "                       n_estimators=50,\n",
    "                       learning_rate=0.1,\n",
    "                       random_state=22)\n",
    "\n",
    "#Entrenamiento\n",
    "ada.fit(x_train,y_train)\n",
    "\n",
    "#Test\n",
    "y_pred=ada.predict(x_test)\n",
    "\n",
    "#Resultado: acuracy\n",
    "print('Accuracy Test: ', accuracy_score(y_test,y_pred))\n",
    "y_pred_train=ada.predict(x_train)\n",
    "print('Accuracy Train: ', accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13487256, 0.16283439, 0.50786648, 0.11645305, 0.07797352])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) GRADIENT BOOSTING\n",
    "\n",
    "Boosting. Los algoritmos se construyen de manera secuencial y no de manera independiente como los vistos hasta este punto. Funciona unicamente con ARBOLES. A DIFERENCIA DEAL aDABOOST ESTE ALGORITMO SE CENTRA EN MINIMIZAR ERRORES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test:  0.7932960893854749\n",
      "Accuracy Train:  0.9115168539325843\n"
     ]
    }
   ],
   "source": [
    "#Creamos el objeto de Boosting (adaboost). Dentro tenemos los parametros del tree (ya que solo funciona con este algoritmo)\n",
    "gradient=GradientBoostingClassifier(n_estimators=50,\n",
    "                                    learning_rate=1,\n",
    "                                    random_state=22)\n",
    "\n",
    "#Entrenamiento\n",
    "gradient.fit(x_train,y_train)\n",
    "\n",
    "#Test\n",
    "y_pred=gradient.predict(x_test)\n",
    "\n",
    "#Resultado: acuracy\n",
    "print('Accuracy Test: ', accuracy_score(y_test,y_pred))\n",
    "y_pred_train=gradient.predict(x_train)\n",
    "print('Accuracy Train: ', accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) XG BOOST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
