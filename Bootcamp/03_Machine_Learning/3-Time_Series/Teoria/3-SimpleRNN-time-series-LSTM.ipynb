{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de series temporales con la clase `SimpleRNN` de Keras\n",
    "### Dr. Tirthajyoti Sarkar, Fremont, CA 94536 ([LinkedIn](https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/), [Github](https://tirthajyoti.github.io))\n",
    "\n",
    "Para más *notebooks* de estilo tutorial sobre aprendizaje profundo, **[aquí tenéis un repositorio de Github](https://github.com/tirthajyoti/Deep-learning-with-Python)**.\n",
    "\n",
    "Para más tutoriales sobre aprendizaje automático en general, **[aquí tenéis un repositorio de Github](https://github.com/tirthajyoti/Machine-Learning-with-Python)**.\n",
    "\n",
    "---\n",
    "### ¿De qué trata este *notebook*?\n",
    "En este *notebook*, mostramos la construcción de una simple red neuronal recurrente (Recurrent Neural Network, *RNN*) usando Keras.\n",
    "\n",
    "Generaremos algunos datos sintéticos de series temporales multiplicando dos señales periódicas/sinusoidales y añadiendo algo de estocasticidad (ruido gaussiano). A continuación, tomaremos una pequeña fracción de los datos y entrenaremos un modelo *RNN* simple con ellos e intentaremos predecir el resto de los datos y ver cómo coinciden las predicciones con la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Las RNN son RRNN especializadas en series temporales y en tratamiento de texto.\n",
    "Llevan un encoding interno de los timestamps utilizados.\n",
    "LSTM: Long Sort Memory Layer. Son capas capaces de trabajar con secuencias de datos.\n",
    "Muy útiles para vídeo o speech recognition. Las neuronas tienen una memoria de los\n",
    "valores anteriores.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de datos\n",
    "N = 3004\n",
    "\n",
    "# Patición train/test.\n",
    "# 25% de los datos (754) para train y el resto para test\n",
    "Tp = 754   \n",
    "\n",
    "np.random.seed(0)\n",
    "t=np.arange(0,N)\n",
    "x=(2*np.sin(0.02*t)*np.sin(0.003*t))+0.5*np.random.normal(size=N)\n",
    "df = pd.DataFrame(x, columns=['Data'])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(df,c='blue')\n",
    "plt.grid(True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir los valores en *train* y *test*\n",
    "\n",
    "Así pues, tomamos sólo el 25% de los datos como muestras de entrenamiento y reservamos el resto de los datos para las pruebas. \n",
    "\n",
    "Observando el gráfico de la serie temporal, pensamos que **no es fácil que un modelo estándar consiga predicciones correctas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.values\n",
    "# Train hasta Tp, hasta 754\n",
    "train, test = values[0:Tp ,:], values[Tp:N,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train data length:\", train.shape)\n",
    "print(\"Test data length:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df.index.values\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(index[0:Tp],train,c='blue')\n",
    "plt.plot(index[Tp:N],test,c='orange',alpha=0.7)\n",
    "plt.legend(['Train','Test'])\n",
    "plt.axvline(Tp, c=\"r\")\n",
    "plt.grid(True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso (o *embedding*)\n",
    "El modelo RNN requiere un valor de $paso$ que contenga número $n$ de elementos como secuencia de entrada.\n",
    "\n",
    "Supongamos x = {1,2,3,4,5,6,7,8,9,10}\n",
    "\n",
    "para $step=1$, la entrada $x$ y su predicción $y$ se convierten en:\n",
    "\n",
    "| $x$ | $y$ |\n",
    "|---|---|\n",
    "| 1  | 2  |\n",
    "| 2  | 3  |\n",
    "| 3  | 4  |\n",
    "| ...  | ...  |\n",
    "| 9  | 10  |\n",
    "\n",
    "para $step=3$, $x$ e $y$ contienen:\n",
    "\n",
    "| $x$ | $y$ |\n",
    "|---|---|\n",
    "| 1,2,3  | 4  |\n",
    "| 2,3,4  | 5  |\n",
    "| 3,4,5  | 6  |\n",
    "| ...  | ...  |\n",
    "| 7,8,9  | 10  |\n",
    "\n",
    "Aquí, elegimos `step=4`. En *RNN* más complejas y, en particular, para el procesamiento de texto, esto también se denomina *embedding size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Básicamente los steps son el número de lags necesarios para realizar las predicciones.\n",
    "Escogemos 4 lags.\n",
    "Se los añade al final repetidos porque luego los eliminará al principio, ya que no puede\n",
    "hacer las predicciones de los lags 1,2,3\n",
    "'''\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "emb_size = 4\n",
    "\n",
    "'''\n",
    "Montamos nuevas features con los lags\n",
    "'''\n",
    "for i in range(1, emb_size+1):\n",
    "    df2['lag' + str(i)] = df2['Data'].shift(i)\n",
    "    \n",
    "df2.dropna(inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "values = df2.values\n",
    "\n",
    "'''\n",
    "Volvemos a montar xtrain, xtest...\n",
    "'''\n",
    "trainX,trainY = values[0:Tp-emb_size ,1:],values[0:Tp-emb_size ,0],\n",
    "testX,testY = values[Tp-emb_size:N-emb_size,1:], values[Tp-emb_size:N-emb_size,0]\n",
    "\n",
    "print(\"Train data length:\", trainX.shape)\n",
    "print(\"Train target length:\", trainY.shape)\n",
    "print(\"Test data length:\", testX.shape)\n",
    "print(\"Test target length:\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "750 son los instantes para entrenar (eq a nº registros)\n",
    "1 x 4 dimensiones de estos dato, como una imagen. Si fuesen mas variables el 1 seria 2 o 3??\n",
    "1 es una fila de datos, necesita ese formato\n",
    "4 los lags para la capa LSTM\n",
    "'''\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el modelo, necesito que los datos tengan la siguiente dimensión:\n",
    "\n",
    "(750, 1, 4)\n",
    "\n",
    "- 750: el número total de trozos \n",
    "- 1: una fila de datos\n",
    "- 4: cada trozo tiene cuatro valores\n",
    "\n",
    "En el caso de una imagen, recordemos con un ejemplo: \n",
    "\n",
    "(750, 28, 28)\n",
    "\n",
    "750 imágenes de resolución 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training data shape:\", trainX.shape,', ',trainY.shape)\n",
    "print(\"Test data shape:\", testX.shape,', ',testY.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Keras\n",
    "\n",
    "- 256 neuronas en la capa de la *RNN*.\n",
    "- 32 neuronas neurons en la capa densamente conectada.\n",
    "- Una única neurona en la capa de salida. Realice la predicción de un único número.\n",
    "- Función de activación *ReLu*.\n",
    "- *Learning Rate*: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "'''\n",
    "embedding es la cantidad de lags utilizada\n",
    "'''\n",
    "def build_simple_rnn(num_units=128, embedding=4,num_dense=32,lr=0.001):\n",
    "    \"\"\"\n",
    "    Builds and compiles a simple RNN model\n",
    "    Arguments:\n",
    "              num_units: Number of units of a the simple RNN layer\n",
    "              embedding: Embedding length\n",
    "              num_dense: Number of neurons in the dense layer followed by the RNN layer\n",
    "              lr: Learning rate (uses RMSprop optimizer)\n",
    "    Returns:\n",
    "              A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Long short term memory\n",
    "    # Esto es capa de entrada + capa con 128 neuronas con su función de activacion\n",
    "    model.add(LSTM(units=num_units, input_shape=(1,embedding), activation=\"relu\"))\n",
    "    model.add(Dense(num_dense, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  #optimizer=RMSprop(lr=lr),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = build_simple_rnn() # Tomando los valores por defecto.\n",
    "#model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parametros LSTM: 4(nm+n**2+n)\n",
    "siendo n = nº neuronas y m = embeddings\n",
    "'''\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una simple clase callback para mostrar un mensaje cada 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Enseña mensaje si la epoch es multiplo de 50 y no ha acabado de entrenar.\n",
    "Cada vez que termina una epoch, keras llama a on_epoch_end()\n",
    "'''\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) % 50 == 0 and epoch>0:\n",
    "            print(\"Epoch number {} done\".format(epoch+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del modelo\n",
    "Con `batch_size` = 16 lo que haríamos es que cogemos los datos de esta forma:\n",
    "\n",
    "- (16, 1, 4)\n",
    "\n",
    "Cogemos 16 trozos de 1 fila con 4 datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(trainX,trainY, \n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[MyCallback()],verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de la función de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Va bajando mucho el RMSE con las epochs, incluso podriamos probar mas epochs\n",
    "'''\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title(\"RMSE loss over epochs\",fontsize=16)\n",
    "plt.plot(np.sqrt(model.history.history['loss']),c='k',lw=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epochs\",fontsize=14)\n",
    "plt.ylabel(\"Root-mean-squared error\",fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones\n",
    "Observa que el modelo se ha ajustado solo con `trainX` y `trainY`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"This is what the model saw\",fontsize=18)\n",
    "# Saca todos los valores de train, la primera columna\n",
    "plt.plot(trainX[:,0],c='blue')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "predicted = np.concatenate((trainPredict,testPredict),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"This is what the model predicted\",fontsize=18)\n",
    "plt.plot(testPredict,c='orange')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación con el conjunto *test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df2.index.values\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Ground truth and prediction together\",fontsize=18)\n",
    "plt.plot(index,df2,c='blue')\n",
    "plt.plot(index,predicted,c='orange',alpha=0.75)\n",
    "plt.legend(['True data','Predicted'],fontsize=15)\n",
    "plt.axvline(df.index[Tp], c=\"r\")\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se distribuyen los errores?\n",
    "Los errores, o residuos, como se denominan en un problema de regresión, pueden representarse gráficamente para ver si siguen alguna distribución específica. En el proceso de generación, inyectamos ruido gaussiano, por lo que esperamos que el error siga el mismo patrón, *si el modelo ha sido capaz de ajustarse a los datos reales correctamente*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Si encontramos patrones raros en los residuos es porque el modelo no se ha ajustado\n",
    "bien a los datos. Habria que probar otras configuraciones/modelos.\n",
    "'''\n",
    "error = predicted[Tp:N]-df2[Tp:N]\n",
    "# Ravel elimina una dimension, lo aplana todo. Como flatten\n",
    "error = np.array(error).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(error,bins=25,edgecolor='k',color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(error,c='blue',alpha=0.75)\n",
    "plt.hlines(y=0,xmin=-50,xmax=2400,color='k',lw=3)\n",
    "plt.xlim(-50,2350)\n",
    "plt.grid(True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorar el modelo\n",
    "\n",
    "Tenga en cuenta que, para que estos experimentos sean razonablemente rápidos, fijaremos el tamaño del modelo para que sea más pequeño que el modelo anterior. Utilizaremos una capa *RNN* de 32 neuronas seguida de una capa densamente conectada de 8 neuronas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variando el tamaño `embedding`/`step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model,trainX,testX):\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    predicted = np.concatenate((trainPredict,testPredict),axis=0)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare(predicted, df2):\n",
    "    index = df2.index.values\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.title(\"Ground truth and prediction together\",fontsize=18)\n",
    "    plt.plot(index,df2,c='blue')\n",
    "    plt.plot(index,predicted,c='orange',alpha=0.75)\n",
    "    plt.legend(['True data','Predicted'],fontsize=15)\n",
    "    plt.axvline(df2.index[Tp], c=\"r\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(step=4):\n",
    "    df2 = df.copy()\n",
    "    emb_size = step\n",
    "    for i in range(1, emb_size+1):\n",
    "        df2['lag' + str(i)] = df2['Data'].shift(i)\n",
    "\n",
    "    df2.dropna(inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    values = df2.values\n",
    "\n",
    "    trainX,trainY = values[0:Tp-emb_size ,1:],values[0:Tp-emb_size ,0],\n",
    "    testX,testY = values[Tp-emb_size:N-emb_size,1:], values[Tp-emb_size:N-emb_size,0]\n",
    "    \n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    \n",
    "    return trainX,testX,trainY,testY,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "def errors(testX, testY):\n",
    "    y_true = testY\n",
    "    y_pred = model.predict(testX)\n",
    "    return mean_absolute_error(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parece que cuanto mayor es la ventana, peor le está viniendo al modelo\n",
    "Mas ruido le mete. Tb es cierto, para 100 epochs\n",
    "'''\n",
    "for s in [2,4,6,8,10,12]:\n",
    "    trainX,testX,trainY,testY,df2 = prepare_data(s)\n",
    "    model = build_simple_rnn(num_units=32,num_dense=8,embedding=s)\n",
    "    batch_size=16\n",
    "    num_epochs = 100\n",
    "    model.fit(trainX,trainY, \n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0)\n",
    "    preds = predictions(model,trainX,testX)\n",
    "    print(\"Embedding size: {}\".format(s))\n",
    "    print(\"MAE:\", errors(testX, testY))\n",
    "    print(\"-\"*100)\n",
    "    plot_compare(preds, df2)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Número de `epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Probemos ahora con una ventana grande (8), y unas cuantas epochs mas\n",
    "'''\n",
    "for e in [100,200,300,400,500]:\n",
    "    trainX, testX, trainY, testY, df2 = prepare_data(6)\n",
    "    model = build_simple_rnn(num_units=32,num_dense=8,embedding=6)\n",
    "    batch_size=16\n",
    "    num_epochs = e\n",
    "    model.fit(trainX,trainY, \n",
    "          epochs=e, \n",
    "          batch_size=batch_size,\n",
    "          verbose=0)\n",
    "    preds = predictions(model,trainX,testX)\n",
    "    print(\"Ran for {} epochs\".format(e))\n",
    "    print(\"MAE:\", errors(testX, testY))\n",
    "    print(\"-\"*100)\n",
    "    plot_compare(preds, df2)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batch size* (tamaño del lote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for b in [4,8,16,32,64,128]:\n",
    "    trainX,testX,trainY,testY, df2 = prepare_data(6)\n",
    "    model = build_simple_rnn(num_units=32,num_dense=8,embedding=6)\n",
    "    batch_size=b\n",
    "    num_epochs = 100\n",
    "    model.fit(trainX,trainY, \n",
    "          epochs=num_epochs, \n",
    "          batch_size=b,\n",
    "          verbose=0)\n",
    "    preds = predictions(model,trainX,testX)\n",
    "    print(\"Ran with batch size: {}\".format(b))\n",
    "    print(\"MAE:\", errors(testX, testY))\n",
    "    print(\"-\"*100)\n",
    "    plot_compare(preds, df2)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "\n",
    "Claramente, se observaron las siguientes tendencias,\n",
    "\n",
    "- Un tamaño de *embedding* demasiado pequeño no es útil, pero un *embedding* muy larga tampoco es eficaz. Un *embedding* de 8 parece buena para estos datos.\n",
    "- Un mayor número de *epochs* no siempre es mejor. Probablemente estamos sufriendo un exceso de ajuste.\n",
    "- Un *batch size* de 32 o 64 parece óptimo.\n",
    "\n",
    "En última instancia, es necesario un ajuste exhaustivo de los hiperparámetros para obtener el mejor rendimiento global."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "## Caso de uso\n",
    "* NETFLIX --> nflx\n",
    "* TESLA --> tsla\n",
    "* GOOGLE --> goog  \n",
    "* META --> meta \n",
    "* APPLE --> aapl \n",
    "* AMAZON --> amzn \n",
    "* IBEX 35 --> ^ibex \n",
    "* PETROLEO --> bz=f \n",
    "* ORO --> gc=f\n",
    "* PLATA --> si=f\n",
    "* EURO/USD --> eurusd=x\n",
    "* REPSOL --> rep.mc\n",
    "* IBERDROLA --> ibe.mc\n",
    "* ETHEREUM --> eth-usd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx = yf.download(\"nflx\", start='2020-01-01', end='2023-06-30')\n",
    "df_nflx = pd.DataFrame(nflx)\n",
    "\n",
    "display(df_nflx.shape, df_nflx.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_nflx.Close\n",
    "train, test = values[:865], values[865:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data length:\", train.shape)\n",
    "print(\"Test data length:\", test.shape)\n",
    "\n",
    "index = df_nflx.index.values\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(index[0:865],train,c='blue')\n",
    "plt.plot(index[865:],test,c='orange',alpha=0.7)\n",
    "plt.legend(['Train','Test'])\n",
    "#plt.axvline(865, c=\"r\")\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_simple_rnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
