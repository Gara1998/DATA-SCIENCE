{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lUTg7PqpE5KW"
   },
   "source": [
    "   \n",
    "## [mlcourse.ai](https://mlcourse.ai) - Curso abierto de *Machine Learning* \n",
    "\n",
    "Autor: [Egor Polusmak](https://www.linkedin.com/in/egor-polusmak/). Traducido y editado por [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/). Este material está sujeto a los términos y condiciones de la licencia [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Se permite su uso libre para cualquier fin no comercial.\n",
    "\n",
    "# <center>Tema 9. Análisis de series temporales en Python</center>\n",
    "## <center>Parte 2. Predecir el futuro con Facebook Prophet</center>\n",
    "\n",
    "La predicción de series temporales encuentra una amplia aplicación en el análisis de datos. Estas son sólo algunas de las predicciones concebibles de tendencias futuras que podrían ser útiles:\n",
    "- El número de servidores que necesitará un servicio en línea el año que viene.\n",
    "- La demanda de un producto de alimentación en un supermercado en un día determinado.\n",
    "- El precio de cierre de mañana de un activo financiero negociable.\n",
    "\n",
    "Otro ejemplo: podemos predecir el rendimiento de un equipo y utilizarlo como referencia: primero para fijar los objetivos del equipo y después para medir el rendimiento real del equipo en relación con la referencia.\n",
    "\n",
    "Existen bastantes métodos diferentes para predecir tendencias futuras, por ejemplo, [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average), [ARCH](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity), [modelos regresivos](https://en.wikipedia.org/wiki/Autoregressive_model), [redes neuronales](https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-1-2-correct-time-series-forecasting-backtesting-9776bfd9e589).\n",
    "\n",
    "En este artículo, examinaremos [Prophet](https://facebook.github.io/prophet/), una biblioteca para la previsión de series temporales lanzada por Facebook y de código abierto el 23 de febrero de 2017. También la probaremos en el problema de predecir el número diario de publicaciones en Medium.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "voH9mGD_E5KX"
   },
   "source": [
    "## Índice\n",
    "\n",
    "1. Introducción\n",
    "2. El modelo de predicción de Prophet\n",
    "3. Práctica con Prophet\n",
    "    * 3.1 Instalación en Python\n",
    "    * 3.2 Conjunto de datos\n",
    "    * 3.3 Análisis visual exploratorio\n",
    "    * 3.4 Realización de una predicción\n",
    "    * 3.5 Evaluación de la calidad de la predicción\n",
    "    * 3.6 Visualización\n",
    "4. Transformación Box-Cox\n",
    "5. Resumen\n",
    "6. Referencias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz0lXaFOE5KX"
   },
   "source": [
    "## 1. Introducción\n",
    "\n",
    "Según el [artículo](https://research.fb.com/prophet-forecasting-at-scale/) de Facebook Research, Prophet se desarrolló inicialmente con el propósito de crear predicciones empresariales de alta calidad. Esta biblioteca trata de abordar las siguientes dificultades comunes a muchas series temporales empresariales:\n",
    "- Efectos estacionales causados por el comportamiento humano: ciclos semanales, mensuales y anuales, caídas y picos en días festivos.\n",
    "- Cambios de tendencia debidos a nuevos productos y acontecimientos del mercado.\n",
    "- Valores atípicos.\n",
    "\n",
    "Los autores afirman que, incluso con la configuración por defecto, en muchos casos su biblioteca produce predicciones tan precisas como las de analistas experimentados.\n",
    "\n",
    "Además, Prophet dispone de una serie de personalizaciones intuitivas y fácilmente interpretables que permiten mejorar gradualmente la calidad del modelo de predicción. Lo que es especialmente importante, estos parámetros son bastante comprensibles incluso para los no expertos en análisis de series temporales, que es un campo de la ciencia de datos que requiere cierta habilidad y experiencia.\n",
    "\n",
    "Por cierto, el artículo original se titula \"Forecasting at Scale\", pero no trata de la escala en el sentido \"habitual\", es decir, abordar los problemas computacionales y de infraestructura de un gran número de programas en funcionamiento. Según los autores, Prophet debe escalar bien en las 3 áreas siguientes:\n",
    "- Accesibilidad a un amplio público de analistas, posiblemente sin profundos conocimientos en series temporales.\n",
    "- Aplicabilidad a una amplia gama de problemas de predicción distintos.\n",
    "- Estimación automatizada del rendimiento de un gran número de predicciones, incluida la señalización de problemas potenciales para su posterior inspección por parte del analista."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ARfg0Il6E5KY"
   },
   "source": [
    "## 2. El modelo de predicción de Prophet\n",
    "\n",
    "Veamos ahora con más detalle cómo funciona Prophet. En su esencia, esta biblioteca utiliza el [modelo de regresión aditiva](https://en.wikipedia.org/wiki/Additive_model) $y(t)$ que comprende las siguientes componentes:\n",
    "\n",
    "$$y(t) = g(t) + s(t) + h(t) + \\epsilon_{t},$$\n",
    "\n",
    "donde:\n",
    "* La tendencia $g(t)$ modela los cambios no periódicos.\n",
    "* La estacionalidad $s(t)$ representa los cambios periódicos.\n",
    "* La componente de vacaciones $h(t)$ aporta información sobre vacaciones y eventos.\n",
    "\n",
    "A continuación, consideraremos algunas propiedades importantes de estas componentes del modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xLnC-LflE5KY"
   },
   "source": [
    "### Tendencia\n",
    "\n",
    "La biblioteca Prophet implementa dos posibles modelos de tendencia para $g(t)$.\n",
    "\n",
    "El primero se denomina *Crecimiento no lineal, saturante*. Se representa en forma de [modelo de crecimiento logístico](https://en.wikipedia.org/wiki/Logistic_function):\n",
    "\n",
    "\n",
    "$$g(t) = \\frac{C}{1+e^{-k(t - m)}},$$\n",
    "\n",
    "donde:\n",
    "* $C$ es la capacidad de carga (es decir, el valor máximo de la curva).\n",
    "* $k$ es la tasa de crecimiento (que representa \"la inclinación\" de la curva).\n",
    "* $m$ es un parámetro de compensación.\n",
    "\n",
    "Esta ecuación logística permite modelizar el crecimiento no lineal con saturación, es decir, cuando la tasa de crecimiento de un valor disminuye con su crecimiento. Uno de los ejemplos típicos sería representar el crecimiento de la audiencia de una aplicación o una página web.\n",
    "\n",
    "En realidad, $C$ y $k$ no son necesariamente constantes y pueden variar con el tiempo. Prophet admite tanto el ajuste automático como el manual de su variabilidad. La biblioteca puede elegir por sí misma los puntos óptimos de los cambios de tendencia ajustándose a los datos históricos suministrados. \n",
    "\n",
    "Además, Prophet permite a los analistas fijar manualmente los puntos de cambio de los valores de la tasa de crecimiento y la capacidad en distintos momentos. Por ejemplo, los analistas pueden tener información sobre fechas de lanzamientos pasados que influyeron de forma destacada en algunos indicadores clave del producto.\n",
    "\n",
    "El segundo modelo de tendencia es un simple *Modelo lineal por partes* con una tasa de crecimiento constante. Es el más adecuado para problemas sin crecimiento saturado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Mwbbr8E5KZ"
   },
   "source": [
    "### Estacionalidad\n",
    "\n",
    "La componente estacional $s(t)$ proporciona un modelo flexible de los cambios periódicos debidos a la estacionalidad semanal y anual.\n",
    "\n",
    "Los datos estacionales semanales se modelan con variables ficticias. Se añaden seis nuevas variables: `monday`, `tuesday`, `wednesday`, `thursday`, `friday`, `saturday`, que toman valores 0 ó 1 según el día de la semana. La característica \"domingo\" no se añade porque sería una combinación lineal de los demás días de la semana, y este hecho tendría un efecto adverso en el modelo.\n",
    "\n",
    "El modelo de estacionalidad anual de Prophet se basa en series de Fourier.\n",
    "\n",
    "Desde la [versión 0.2](https://github.com/facebook/prophet) también puede utilizar *sub-series temporales diarias* y realizar *previsiones subdiarias*, así como emplear la nueva función de *estacionalidad diaria*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QMnfe-G0E5KZ"
   },
   "source": [
    "### Días festivos y eventos\n",
    "\n",
    "La componente $h(t)$ representa los días anormales previsibles del año, incluidos los de calendario irregular, por ejemplo, los Black Fridays.\n",
    "\n",
    "Para utilizar esta función, el analista debe proporcionar una lista personalizada de eventos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KPPTsUBZE5Ka"
   },
   "source": [
    "### Error\n",
    "\n",
    "El término de error $\\epsilon(t)$ representa información no reflejada en el modelo. Normalmente se modela como ruido distribuido normalmente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YFc28oD7E5Ka"
   },
   "source": [
    "### Prophet Benchmarking\n",
    "\n",
    "Para una descripción detallada del modelo y los algoritmos de Prophet, consulte el artículo [\"Forecasting at scale\"](https://peerj.com/preprints/3190/) de Sean J. Taylor y Benjamin Letham.\n",
    "\n",
    "Los autores también compararon su biblioteca con otros métodos de predicción de series temporales. Utilizaron [Mean Absolute Percentage Error (MAPE)](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) como medida de la precisión de la predicción. En esta investigación, Prophet ha mostrado un error de predicción sustancialmente menor que los otros modelos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NzoU__ozE5Kb"
   },
   "source": [
    "<img src=\"img/topic9_benchmarking_prophet.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kZJZ3ySNE5Kb"
   },
   "source": [
    "Veamos con más detalle cómo se ha medido la calidad de las predicciones en el artículo. Para ello, necesitaremos la fórmula del error porcentual absoluto medio (*MAPE*).\n",
    "\n",
    "Sea $y_{i}$ el *valor real (histórico)* y $\\hat{y}_{i}$ el *valor predicho* por nuestro modelo.\n",
    "\n",
    "A continuación, $e_{i} = y_{i} - \\hat{y}_{i}$ es el *error de predicción* y $p_{i} = \\frac{e_{i}}{y_{i}}$ es el *error relativo de predicción*.\n",
    "\n",
    "Definimos\n",
    "\n",
    "$$MAPE = mean\\big(\\left |p_{i}\\right |\\big)$$\n",
    "\n",
    "*MAPE* se utiliza ampliamente como medida de la precisión de la predicción porque expresa el error como porcentaje y, por tanto, puede utilizarse en evaluaciones de modelos en diferentes conjuntos de datos.\n",
    "\n",
    "Además, al evaluar un algoritmo de predicción, puede resultar útil calcular [*MAE* (Error Medio Absoluto)](https://en.wikipedia.org/wiki/Mean_absolute_error) para tener una idea de los errores en números absolutos. Utilizando los componentes definidos anteriormente, su ecuación será\n",
    "\n",
    "$$MAE = mean\\big(\\left |e_{i}\\right |\\big)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdk75ZfxE5Kc"
   },
   "source": [
    "Unas palabras sobre los algoritmos con los que se comparó Prophet. La mayoría de ellos son bastante sencillos y suelen utilizarse como referencia para otros modelos:\n",
    "* `naive` (ingenuo) es un enfoque de predicción simplista en el que predecimos todos los valores futuros basándonos únicamente en la observación en el último momento disponible.\n",
    "* El modelo `snaive` (ingenuo estacional) realiza predicciones constantes teniendo en cuenta la información sobre la estacionalidad. Por ejemplo, en el caso de datos estacionales semanales, para cada lunes futuro se predeciría el valor del último lunes, y para todos los martes futuros se utilizaría el valor del último martes, y así sucesivamente.\n",
    "* `mean` utiliza el valor medio de los datos como predicción.\n",
    "* `arima` significa *Media Móvil Autorregresiva Integrada*, véase [Wikipedia](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) para más detalles.\n",
    "* `ets` significa *Suavizado exponencial*, véase [Wikipedia](https://en.wikipedia.org/wiki/Exponential_smoothing) para más información."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r6QduJTlE5Kc"
   },
   "source": [
    "## 3. Práctica con Prophet\n",
    "\n",
    "### 3.1 Instalación en Python\n",
    "\n",
    "En primer lugar, necesitas instalar la librería. Prophet está disponible para Python y R. La elección dependerá de tus preferencias personales y de los requisitos del proyecto. En este *notebook* utilizaremos Python.\n",
    "\n",
    "En Python puedes instalar Prophet usando PyPI:\n",
    "\n",
    "```\n",
    "$ pip install fbprophet\n",
    "```\n",
    "\n",
    "En R puede encontrar el paquete CRAN correspondiente. Consulta la [documentación](https://facebookincubator.github.io/prophet/docs/installation.html) para más detalles.\n",
    "\n",
    "Importemos los módulos que necesitaremos e inicialicemos nuestro entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ksf_V_uE5Kd"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VVwIaHxGE5Kg"
   },
   "source": [
    "### 3.2 *Dataset*\n",
    "\n",
    "Vamos a predecir el número de posts diarios publicados en [Medium](https://medium.com/).\n",
    "\n",
    "Primero, cargamos nuestro *dataset*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medium_posts.csv', sep='\\t')\n",
    "display(df.shape, df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NW-2BSRbE5Ki"
   },
   "source": [
    "A continuación, omitimos todas las columnas excepto `published` y `url`. La primera corresponde a la dimensión temporal, mientras que la segunda identifica de forma exclusiva una entrada por su URL. De este modo, eliminamos los posibles duplicados y los valores que faltan en los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFaYhg-kE5Ki"
   },
   "outputs": [],
   "source": [
    "df = df[['published', 'url']].dropna().drop_duplicates()\n",
    "display(df.shape, df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZHymv9xE5Kl"
   },
   "source": [
    "A continuación, tenemos que convertir `published` al formato datetime porque por defecto `pandas` trata este campo como string-valued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aijp2HDBE5Kl"
   },
   "outputs": [],
   "source": [
    "df['published'] = pd.to_datetime(df['published'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vrlyvBYYE5Ko"
   },
   "source": [
    "Ordenemos el marco de datos por tiempo y echemos un vistazo a lo que tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucArYdZ9E5Ko",
    "outputId": "b6046139-e918-439b-ad3f-4f48b5cfe9a4"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['published']).head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hWlDlJMgE5Ks"
   },
   "source": [
    "La fecha de publicación de Medium fue el 15 de agosto de 2012. Pero, como se puede ver en los datos anteriores, hay al menos varias filas con fechas de publicación muy anteriores. De alguna manera han aparecido en nuestro conjunto de datos, pero difícilmente son legítimas. Nos limitaremos a recortar nuestra serie temporal para conservar únicamente las filas que se sitúan en el periodo comprendido entre el 15 de agosto de 2012 y el 25 de junio de 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psimVAK_E5Ks",
    "outputId": "edab7fd4-4403-4db4-ddb4-e1702211c598"
   },
   "outputs": [],
   "source": [
    "df = df[(df['published'] > '2012-08-15') & (df['published'] < '2017-06-26')].sort_values(by=['published'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paYQ5v-YE5Kv",
    "outputId": "34fa7fd8-1132-4b56-f98f-f2a298ee2430"
   },
   "outputs": [],
   "source": [
    "df.tail(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "P_lPudV-E5Kw"
   },
   "source": [
    "Como vamos a predecir el número de entradas publicadas, agregaremos y contaremos las entradas únicas en cada momento. Denominaremos a la nueva columna correspondiente `posts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5GwNkXVE5Kx"
   },
   "outputs": [],
   "source": [
    "aggr_df = df.groupby('published')[['url']].count()\n",
    "aggr_df.columns = ['posts']\n",
    "display(aggr_df.shape, aggr_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LNI60BhbE5Kz"
   },
   "source": [
    "En esta práctica, nos interesa el número de mensajes **al día**. Pero en este momento todos nuestros datos están divididos en intervalos de tiempo irregulares que son inferiores a un día. Esto se denomina *serie temporal subdiaria*. Para verlo, imprimamos las 3 primeras filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrwvBi85E5K0",
    "outputId": "c80c2049-23d6-4f73-e5cc-023094255156"
   },
   "outputs": [],
   "source": [
    "aggr_df.head(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VvAlAU95E5K2"
   },
   "source": [
    "Para solucionarlo, tenemos que agregar los recuentos de entradas por \"intervalos\" de un tamaño de fecha. En el análisis de series temporales, este proceso se denomina *remuestreo*. Y si *reducimos* la tasa de muestreo de los datos se suele llamar *downsampling*.\n",
    "\n",
    "Por suerte, `pandas` tiene una funcionalidad incorporada para esta tarea. Vamos a remuestrear nuestro índice de tiempo a intervalos de 1 día:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HZk0oqWE5K2",
    "outputId": "c926de76-82eb-446b-e5a9-7b784bc1c09d"
   },
   "outputs": [],
   "source": [
    "daily_df = aggr_df.resample('D').apply(sum)\n",
    "daily_df.head(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sbcoxKhbE5K4"
   },
   "source": [
    "### 3.3 Análisis visual exploratorio\n",
    "\n",
    "Como siempre, puede resultar útil e instructivo observar una representación gráfica de los datos.\n",
    "\n",
    "Crearemos un gráfico de series temporales para todo el intervalo de tiempo. La visualización de los datos durante un periodo de tiempo tan largo puede dar pistas sobre la estacionalidad y las desviaciones anormales llamativas.\n",
    "\n",
    "En primer lugar, importamos e inicializamos la librería `Plotly`, que permite crear bonitos gráficos interactivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkOH7_yME5K4",
    "outputId": "99a761ab-c1b4-4036-8141-98a14d1619bb"
   },
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "# Initialize plotly\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i1jOuUiBE5K6"
   },
   "source": [
    "También definimos una función de ayuda, que trazará nuestros *datasets* a lo largo del *notebook*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5TVCqeGBE5K6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plotly_df(df, title=''):\n",
    "    \"\"\"Visualize all the dataframe columns as line plots.\"\"\"\n",
    "    common_kw = dict(x=df.index, mode='lines')\n",
    "    data = [go.Scatter(y=df[c], name=c, **common_kw) for c in df.columns]\n",
    "    layout = dict(title=title)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca0VQ5yME5K9"
   },
   "source": [
    "Intentemos trazar nuestro conjunto de datos *tal cual*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvQOxaHnE5K9",
    "outputId": "cf0edf80-4f54-48df-e066-b408650911be"
   },
   "outputs": [],
   "source": [
    "plotly_df(daily_df, title='Posts on Medium (daily)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q9OD8syE5K_"
   },
   "source": [
    "Los datos de alta frecuencia pueden ser bastante difíciles de analizar. Incluso con la posibilidad de hacer zoom que ofrece `Plotly`, es difícil deducir algo significativo de este gráfico, además de la prominente tendencia al alza y la aceleración.\n",
    "\n",
    "Para reducir el ruido, volveremos a muestrear los recuentos de mensajes en intervalos semanales. Además del *binning*, otras posibles técnicas de reducción del ruido son el [Suavizado de medias móviles](https://en.wikipedia.org/wiki/Moving_average) y el [Suavizado exponencial](https://en.wikipedia.org/wiki/Exponential_smoothing), entre otras.\n",
    "\n",
    "Guardamos nuestro *dataset* en una variable separada porque en esta práctica sólo trabajaremos con series diarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "cfz7c-lDE5K_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "weekly_df = daily_df.resample('W').apply(sum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3PSAZKpE5LB"
   },
   "source": [
    "Finalmente, visualizamos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K_C3jMCE5LB",
    "outputId": "27a1c101-29da-4dc0-fbc0-81f1e2110247"
   },
   "outputs": [],
   "source": [
    "plotly_df(weekly_df, title='Posts on Medium (weekly)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tcj3t405E5LD"
   },
   "source": [
    "Este gráfico con muestreo reducido resulta algo mejor para la percepción de un analista.\n",
    "\n",
    "Una de las funciones más útiles que ofrece `Plotly` es la posibilidad de sumergirse rápidamente en distintos periodos de la línea temporal para comprender mejor los datos y encontrar pistas visuales sobre posibles tendencias, efectos periódicos e irregulares. \n",
    "\n",
    "Por ejemplo, hacer zoom en un par de años consecutivos nos muestra puntos temporales correspondientes a las fiestas navideñas, que influyen enormemente en los comportamientos humanos.\n",
    "\n",
    "Ahora, vamos a omitir los primeros años de observaciones, hasta 2015. En primer lugar, no contribuirán mucho a la calidad de las previsiones en 2017. En segundo lugar, es probable que estos primeros años, con un número muy bajo de publicaciones al día, aumenten el ruido en nuestras predicciones, ya que el modelo se vería obligado a ajustar estos datos históricos anómalos junto con datos más relevantes e indicativos de los últimos años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrPBQIlvE5LD",
    "outputId": "f7e1f938-7a7b-4dae-ddef-a5e7e32b7ee3"
   },
   "outputs": [],
   "source": [
    "daily_df = daily_df.loc[daily_df.index >= '2015-01-01']\n",
    "daily_df.head(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w1kOY-ShE5LF"
   },
   "source": [
    "En resumen, del análisis visual se observa que nuestro conjunto de datos es no estacionario y presenta una marcada tendencia creciente. También muestra estacionalidad semanal y anual y un número de días anormales cada año."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wSM0CpT2E5LF"
   },
   "source": [
    "### 3.4 Ejecutar una predicción\n",
    "\n",
    "La API de Prophet es muy similar a la que puedes encontrar en `sklearn`. Primero creamos un modelo, luego llamamos al método `fit`, y, finalmente, hacemos una predicción. La entrada del método `fit` es un `DataFrame` con dos columnas:\n",
    "* `ds` (*datestamp*) debe ser de tipo `date` o `datetime`.\n",
    "* `y` es un valor numérico que queremos predecir.\n",
    "\n",
    "Para empezar, importaremos la librería y silenciaremos los mensajes de diagnóstico sin importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qHrYm65E5LF"
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LX2ATVT6E5LH"
   },
   "source": [
    "Convirtamos nuestro *dataset* al formato requerido por Prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "jnoaiEFEE5LH",
    "outputId": "3a7330c2-5629-455e-c9d0-f03958e20edc"
   },
   "outputs": [],
   "source": [
    "df = daily_df.reset_index()\n",
    "df.columns = ['ds', 'y']\n",
    "display(df.head(n=3), df.tail(n=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBHyMivE5LJ"
   },
   "source": [
    "Los autores de la biblioteca suelen aconsejar hacer predicciones basadas en al menos varios meses, e idealmente, más de un año de datos históricos. Por suerte, en nuestro caso tenemos más de un par de años de datos para ajustar el modelo.\n",
    "\n",
    "Para medir la calidad de nuestra predicción, tenemos que dividir nuestro conjunto de datos en la *parte histórica*, que es la primera y mayor porción de nuestros datos, y la *parte de predicción*, que se situará al final de la línea temporal. Eliminaremos el último mes del conjunto de datos para utilizarlo posteriormente como objetivo de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1FjuduyE5LJ",
    "outputId": "aeb82376-c80e-44b5-b506-53010382b556"
   },
   "outputs": [],
   "source": [
    "prediction_size = 30\n",
    "df.ds = df.ds.astype(str).str.split(' ').str[0]\n",
    "df.ds = pd.to_datetime(df.ds)\n",
    "train_df = df[:-prediction_size]\n",
    "display(train_df.head(n=3), train_df.tail(n=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EWW-2ZffE5LL"
   },
   "source": [
    "Ahora necesitamos crear un nuevo objeto `Prophet`. Aquí podemos pasar los parámetros del modelo en el constructor. Pero en este *notebook* vamos a utilizar los valores por defecto. Luego entrenamos nuestro modelo invocando su método `fit` en nuestro conjunto de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElpttBXBE5LL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Prophet()  # Si da el siguiente error: \"AttributeError: 'Prophet' object has no attribute 'stan_backend'\". Ejecutad en la terminal: pip install --upgrade pystan.\n",
    "m.fit(train_df);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QctNFxWJE5LN"
   },
   "source": [
    "Usando el método de ayuda `Prophet.make_future_dataframe`, creamos un *dataset* que contendrá todas las fechas del histórico y también se extenderá hacia el futuro para esos 30 días que dejamos fuera antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3p4-bQtE5LO",
    "outputId": "79a35655-091a-4b20-e357-6bf40bb3137a"
   },
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=prediction_size)\n",
    "future.tail(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RunPAGZQE5LP"
   },
   "source": [
    "Predecimos valores con `Prophet` introduciendo las fechas para las que queremos crear una predicción. Si también proporcionamos las fechas históricas (como en nuestro caso), además de la predicción obtendremos un ajuste dentro de la muestra para la historia. Llamemos al método `predict` del modelo con nuestro *dataset* `future` como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aECBUBXRE5LQ",
    "outputId": "bc0a93cb-8de3-4842-97b1-1452cfc13108"
   },
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast.tail(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1DVgTd6VE5LR"
   },
   "source": [
    "En el *dataset* resultante puede ver muchas columnas que caracterizan la predicción, incluidos los componentes de tendencia y estacionalidad, así como sus intervalos de confianza. La propia predicción se almacena en la columna `yhat`.\n",
    "\n",
    "La biblioteca Prophet tiene sus propias herramientas de visualización que nos permiten evaluar rápidamente el resultado.\n",
    "\n",
    "En primer lugar, existe un método llamado `Prophet.plot` que traza todos los puntos de la predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InzduNjDE5LR",
    "outputId": "ec4a01d7-2dd8-480e-fde6-fa568d2a65de"
   },
   "outputs": [],
   "source": [
    "m.plot(forecast);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GxqEXpGQE5LT"
   },
   "source": [
    "Este gráfico no parece muy informativo. La única conclusión definitiva que podemos sacar aquí es que el modelo trató muchos de los puntos de datos como valores atípicos.\n",
    "\n",
    "La segunda función `Prophet.plot_components` podría ser mucho más útil en nuestro caso. Nos permite observar por separado las distintas componentes del modelo: tendencia, estacionalidad anual y semanal. Además, si proporciona información sobre días festivos y eventos a su modelo, también se mostrarán en este gráfico.\n",
    "\n",
    "Vamos a probarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJj20SHME5LU",
    "outputId": "ac19564d-aa9d-4043-e941-a50aef7367c3"
   },
   "outputs": [],
   "source": [
    "m.plot_components(forecast);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc_-qTrqE5LV"
   },
   "source": [
    "Como se puede ver en el gráfico de tendencias, Prophet hizo un buen trabajo al encajar el crecimiento acelerado de nuevos posts a finales de 2016. El gráfico de estacionalidad semanal llega a la conclusión de que normalmente hay menos entradas nuevas los sábados y domingos que el resto de días de la semana. En el gráfico de estacionalidad anual se observa un descenso destacado el día de Navidad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FBAwb_-8E5LV"
   },
   "source": [
    "### 3.5 Evaluación de las predicciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8IAvjmW3E5LW"
   },
   "source": [
    "Evaluemos la calidad del algoritmo calculando la métrica del error para los últimos 30 días que hemos predicho. Para ello, necesitaremos las observaciones $y_i$ y los correspondientes valores predichos $\\hat{y}_i$.\n",
    "\n",
    "Vamos a ver en el objeto `forecast` que la biblioteca creado para nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j8kDjhLE5LW",
    "outputId": "9e7714db-08b0-44e4-da1c-0a440975e93e"
   },
   "outputs": [],
   "source": [
    "print(', '.join(forecast.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YggjAKDjE5LY"
   },
   "source": [
    "Podemos ver que este *dataset* contiene toda la información que necesitamos excepto los valores históricos. Necesitamos unir el objeto `forecast` con los valores reales `y` del conjunto de datos original `df`. Para ello definiremos una función de ayuda que reutilizaremos más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "G0oWuyWfE5LZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_comparison_dataframe(historical, forecast):\n",
    "    \"\"\"Join the history with the forecast.\n",
    "    \n",
    "       The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.\n",
    "    \"\"\"\n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhj1gdB0E5La"
   },
   "source": [
    "\n",
    "Apliquemos esta función a nuestra última predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocv5wSAGE5La",
    "outputId": "14bd4f1a-b57e-4834-ee30-0e8d82d37556"
   },
   "outputs": [],
   "source": [
    "cmp_df = make_comparison_dataframe(df, forecast)\n",
    "display(df, forecast, cmp_df.tail(n=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AzG3kwsRE5Lc"
   },
   "source": [
    "También vamos a definir una función de ayuda que utilizaremos para medir la calidad de nuestras predicciones con las medidas de error *MAPE* y *MAE*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CJzWqfm_E5Lc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_forecast_errors(df, prediction_size):\n",
    "    \"\"\"Calculate MAPE and MAE of the forecast.\n",
    "    \n",
    "       Args:\n",
    "           df: joined dataset with 'y' and 'yhat' columns.\n",
    "           prediction_size: number of days at the end to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Now we calculate the values of e_i and p_i according to the formulas given in the article above.\n",
    "    df['e'] = df['y'] - df['yhat']\n",
    "    df['p'] = 100 * df['e'] / df['y']\n",
    "    \n",
    "    # Recall that we held out the values of the last `prediction_size` days\n",
    "    # in order to predict them and measure the quality of the model. \n",
    "    \n",
    "    # Now cut out the part of the data which we made our prediction for.\n",
    "    predicted_part = df[-prediction_size:]\n",
    "    \n",
    "    # Define the function that averages absolute error values over the predicted part.\n",
    "    error_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n",
    "    \n",
    "    # Now we can calculate MAPE and MAE and return the resulting dictionary of errors.\n",
    "    return {'MAPE': error_mean('p'), 'MAE': error_mean('e')}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OMB-yrDiE5Le"
   },
   "source": [
    "Usemos nuestra función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hddFj-atE5Le",
    "outputId": "1ea2a6db-1fe3-4d83-9da3-84231f0a27c5"
   },
   "outputs": [],
   "source": [
    "for err_name, err_value in calculate_forecast_errors(cmp_df, prediction_size).items():\n",
    "    print(err_name, err_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r_sUPo0pE5Lg"
   },
   "source": [
    "Como resultado, el error relativo de nuestra predicción (*MAPE*) se sitúa en torno al 22.72%, y por término medio nuestro modelo se equivoca en 70.45 posts (*MAE*)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SjjZusOgE5Lg"
   },
   "source": [
    "### 3.6 Visualización\n",
    "\n",
    "Vamos a crear nuestra propia visualización del modelo construido por Prophet. Comprenderá los valores reales, la predicción y los intervalos de confianza.\n",
    "\n",
    "En primer lugar, representaremos los datos de un periodo de tiempo más corto para que los puntos de datos sean más fáciles de distinguir. En segundo lugar, mostraremos el rendimiento del modelo sólo para el periodo que hemos predicho, es decir, los últimos 30 días. Parece que con estas dos medidas obtendremos un gráfico más legible.\n",
    "\n",
    "En tercer lugar, vamos a utilizar `Plotly` para hacer nuestro gráfico interactivo, que es ideal para explorar.\n",
    "\n",
    "Definiremos una función de ayuda personalizada `show_forecast` y la llamaremos (para más información sobre su funcionamiento, consulta los comentarios del código y la [documentación](https://plot.ly/python/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdhQTOTYE5Lh",
    "outputId": "1e8f631f-1a8a-4051-a2e7-c2dbb997e897"
   },
   "outputs": [],
   "source": [
    "def show_forecast(cmp_df, num_predictions, num_values, title):\n",
    "    \"\"\"Visualize the forecast.\"\"\"\n",
    "    \n",
    "    def create_go(name, column, num, **kwargs):\n",
    "        points = cmp_df.tail(num)\n",
    "        args = dict(name=name, x=points.index, y=points[column], mode='lines')\n",
    "        args.update(kwargs)\n",
    "        return go.Scatter(**args)\n",
    "    \n",
    "    lower_bound = create_go('Lower Bound', 'yhat_lower', num_predictions,\n",
    "                            line=dict(width=0),\n",
    "                            marker=dict(color='DarkSlateGrey'))\n",
    "    upper_bound = create_go('Upper Bound', 'yhat_upper', num_predictions,\n",
    "                            line=dict(width=0),\n",
    "                            marker=dict(color='DarkSlateGrey'),\n",
    "                            fillcolor='rgba(68, 68, 68, 0.3)', \n",
    "                            fill='tonexty')\n",
    "    forecast = create_go('Forecast', 'yhat', num_predictions,\n",
    "                         line=dict(color='rgb(31, 119, 180)'))\n",
    "    actual = create_go('Actual', 'y', num_values,\n",
    "                       marker=dict(color=\"red\"))\n",
    "    \n",
    "    # In this case the order of the series is important because of the filling\n",
    "    data = [lower_bound, upper_bound, forecast, actual]\n",
    "\n",
    "    layout = go.Layout(yaxis=dict(title='Posts'), title=title, showlegend = False)\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "show_forecast(cmp_df, prediction_size, 100, 'New posts on Medium')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zekgqmXyE5Li"
   },
   "source": [
    "A primera vista, la predicción de los valores medios por nuestro modelo parece sensata. El elevado valor de *MAPE* que obtuvimos anteriormente puede explicarse por el hecho de que el modelo no consiguió captar el aumento de la amplitud pico a pico de la estacionalidad débil.\n",
    "\n",
    "Además, podemos concluir del gráfico anterior que muchos de los valores reales se sitúan fuera del intervalo de confianza. Puede que Prophet no sea adecuado para series temporales con varianza inestable, al menos cuando se utilizan los ajustes por defecto. Intentaremos solucionarlo aplicando una transformación a nuestros datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hSJhjWJSE5Lj"
   },
   "source": [
    "## 4. Transformación Box-Cox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GIRlrH_HE5Lj"
   },
   "source": [
    "Hasta ahora hemos utilizado Prophet con los parámetros por defecto y los datos originales. Dejaremos solo los parámetros del modelo. Pero, a pesar de ello, aún tenemos margen de mejora. En esta sección, aplicaremos la [transformación Box-Cox](http://onlinestatbook.com/2/transformations/box-cox.html) a nuestra serie original. Veamos a dónde nos lleva.\n",
    "\n",
    "Unas palabras sobre esta transformación. Se trata de una transformación monótona de datos que puede utilizarse para estabilizar la varianza. Utilizaremos la transformación de Box-Cox de un parámetro, que se define mediante la siguiente expresión:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  boxcox^{(\\lambda)}(y_{i}) = \\begin{cases}\n",
    "    \\frac{\\displaystyle y_{i}^{\\lambda} - 1}{\\displaystyle \\lambda} &, \\text{if $\\lambda \\neq 0$}.\\\\\n",
    "    ln(y_{i}) &, \\text{if $\\lambda = 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    ":\n",
    "Necesitaremos implementar la inversa de esta función para poder restaurar la escala original de los datos. Es fácil ver que la inversa se define como:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  invboxcox^{(\\lambda)}(y_{i}) = \\begin{cases}\n",
    "    e^{\\left (\\frac{\\displaystyle ln(\\lambda y_{i} + 1)}{\\displaystyle \\lambda} \\right )} &, \\text{if $\\lambda \\neq 0$}.\\\\\n",
    "    e^{y_{i}} &, \\text{if $\\lambda = 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "La función correspondiente en Python se implementa del siguiente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwSaYm22E5Lk"
   },
   "outputs": [],
   "source": [
    "def inverse_boxcox(y, lambda_):\n",
    "    return np.exp(y) if lambda_ == 0 else np.exp(np.log(lambda_ * y + 1) / lambda_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4gNmi3c5E5Lm"
   },
   "source": [
    "En primer lugar, preparamos nuestro conjunto de datos estableciendo su índice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8yEKOxSE5Ln"
   },
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy().set_index('ds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3FCBZHzXE5Lq"
   },
   "source": [
    "A continuación, aplicamos la función `stats.boxcox` de `Scipy`, que aplica la transformación Box-Cox. En nuestro caso devolverá dos valores. El primero es la serie transformada y el segundo es el valor encontrado de $\\lambda$ que es óptimo en términos de máxima log-verosimilitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzni1jL8E5Lq"
   },
   "outputs": [],
   "source": [
    "train_df2['y'], lambda_prophet = stats.boxcox(train_df2['y'])\n",
    "train_df2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Y74jXjYrE5Lu"
   },
   "source": [
    "Creamos un nuevo modelo `Prophet` y repetimos el ciclo de ajuste-predicción que ya hemos realizado anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQMSvEeiE5Lu"
   },
   "outputs": [],
   "source": [
    "m2 = Prophet()\n",
    "m2.fit(train_df2)\n",
    "future2 = m2.make_future_dataframe(periods=prediction_size)\n",
    "forecast2 = m2.predict(future2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UVWG5InoE5Lv"
   },
   "source": [
    "En este punto, tenemos que revertir la transformación Box-Cox con nuestra función inversa y el valor conocido de $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhA-LbL6E5Lw"
   },
   "outputs": [],
   "source": [
    "for column in ['yhat', 'yhat_lower', 'yhat_upper']:\n",
    "    forecast2[column] = inverse_boxcox(forecast2[column], lambda_prophet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uQLWJC75E5Lx"
   },
   "source": [
    "Aquí reutilizaremos nuestras herramientas para hacer el *dataset* de comparación y calcular los errores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TDsh9m1E5Ly",
    "outputId": "93c95a7a-723c-4aba-a6de-757de9f8a48c"
   },
   "outputs": [],
   "source": [
    "cmp_df2 = make_comparison_dataframe(df, forecast2)\n",
    "for err_name, err_value in calculate_forecast_errors(cmp_df2, prediction_size).items():\n",
    "    print(err_name, err_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Al1ex3E5Lz"
   },
   "source": [
    "Así pues, podemos afirmar sin lugar a dudas que ha aumentado la calidad del modelo.\n",
    "\n",
    "Por último, vamos a comparar nuestro rendimiento anterior con los últimos resultados. Obsérvese que utilizamos `prediction_size` como tercer parámetro para ampliar el intervalo de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj23KmfVE5Lz",
    "outputId": "a0cec00f-3f62-4ed1-a437-633ca171e9a4"
   },
   "outputs": [],
   "source": [
    "show_forecast(cmp_df, prediction_size, 100, 'No transformations')\n",
    "show_forecast(cmp_df2, prediction_size, 100, 'Box–Cox transformation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YxfgqLaFE5L0"
   },
   "source": [
    "Vemos que la predicción de cambios semanales del segundo gráfico se acerca mucho más a los valores reales actuales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t16kwyswE5L1"
   },
   "source": [
    "## 5. Resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F9qNYOyCE5L1"
   },
   "source": [
    "Hemos echado un vistazo a *Prophet*, una biblioteca de predicción de código abierto orientada específicamente a las series temporales empresariales. También hemos realizado algunas prácticas de predicción de series temporales.\n",
    "\n",
    "Como hemos visto, la biblioteca Prophet no hace maravillas, y sus predicciones out-of-box no son [ideales](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization). Sigue correspondiendo al científico de datos explorar los resultados de la predicción, ajustar los parámetros del modelo y transformar los datos cuando sea necesario.\n",
    "\n",
    "Sin embargo, esta biblioteca es fácil de usar y personalizar. La sola posibilidad de tener en cuenta los días anormales que el analista conoce de antemano podría marcar la diferencia en algunos casos.\n",
    "\n",
    "En definitiva, merece la pena que la biblioteca Prophet forme parte de su caja de herramientas analíticas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1GAc5mE5L1"
   },
   "source": [
    "## 6. Referencias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_jE1cdoJE5L2"
   },
   "source": [
    "* Repositorio oficial [Prophet](https://github.com/facebookincubator/prophet) en GitHub.\n",
    "* Documentación oficial [Prophet](https://facebookincubator.github.io/prophet/docs/quick_start.html).\n",
    "* Sean J. Taylor, Benjamin Letham [\"Forecasting at scale\"](https://facebookincubator.github.io/prophet/static/prophet_paper_20170113.pdf) - artículo científico en el que se explica el algoritmo que sienta las bases de `Prophet`.\n",
    "* [Forecasting Website Traffic Using Facebook's Prophet Library](http://pbpython.com/prophet-overview.html) - descripción general de `Prophet` con un ejemplo de previsión del tráfico de un sitio web.\n",
    "* Rob J. Hyndman, George Athanasopoulos [\"Forecasting: principles and practice\"](https://www.otexts.org/fpp) - un libro en línea muy bueno sobre predicción de series temporales."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "timeseries_facebook_prophet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
